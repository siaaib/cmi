{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Child Mind Sleep States\n","\n","I'll introduce this notebook as a way to use the KLDivLoss, as well as an architecture that is a mix of GRU cells and the UNET architecture.  It can be improved, and done rather easily.  I will discuss the architecture below.\n","\n","I have used the Kullback-Leibler divergence loss from torch.  \n","\n","`L(y_pred, y_true) = y_true * (log y_true - log y_pred)`\n","\n","\n","My reasoning:\n","\n","1. It is easily interpretable as a probability distribution\n","2. Ensembles are also easy to interpret, no matter how differently they are trained.\n","\n","We are going to predict the the critical points, i.e. where the onset and wakeup events happen.  We will take as the target a gaussian of a tunable width centered around the actual point.\n","\n","\n","This, of course, draws heavily from @werus23: \n","\n","https://www.kaggle.com/code/werus23/sleep-critical-point-train/notebook\n","\n","https://www.kaggle.com/code/werus23/sleep-critical-point-infer?scriptVersionId=147143158\n","\n","Which is based on the wonderful discussion pose of @tolgadincer: \n","\n","https://www.kaggle.com/competitions/child-mind-institute-detect-sleep-states/discussion/441470\n","\n","\n","There could be bugs and other issues in here.  No promises!"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Saved fold 0 to /home/siavash/random/dsseg/run/conf/split/fold_0.yaml\n","Saved fold 1 to /home/siavash/random/dsseg/run/conf/split/fold_1.yaml\n","Saved fold 2 to /home/siavash/random/dsseg/run/conf/split/fold_2.yaml\n","Saved fold 3 to /home/siavash/random/dsseg/run/conf/split/fold_3.yaml\n","Saved fold 4 to /home/siavash/random/dsseg/run/conf/split/fold_4.yaml\n"]}],"source":["import pandas as pd\n","from sklearn.model_selection import KFold\n","import yaml\n","\n","df = pd.read_csv('/home/siavash/random/data/train_events.csv')\n","ids = df.series_id.unique()\n","skf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","for fold_num, (train_index, valid_index) in enumerate(skf.split(ids), start=0):\n","    train, valid = ids[train_index], ids[valid_index]\n","    fold_data = {\n","        'train_series_ids': [str(x) for x in train.tolist()],\n","        'valid_series_ids': [str(x) for x in valid.tolist()]\n","    }\n","    \n","    # Define the YAML filename for this fold\n","    yaml_filename = f'/home/siavash/random/dsseg/run/conf/split/fold_{fold_num}.yaml'\n","\n","    # Write the fold_data dictionary to a YAML file\n","    with open(yaml_filename, 'w') as file:\n","        yaml.dump(fold_data, file)\n","\n","    print(f'Saved fold {fold_num} to {yaml_filename}')\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>series_id</th>\n","      <th>night</th>\n","      <th>event</th>\n","      <th>step</th>\n","      <th>timestamp</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>038441c925bb</td>\n","      <td>1</td>\n","      <td>onset</td>\n","      <td>4992.0</td>\n","      <td>2018-08-14T22:26:00-0400</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>038441c925bb</td>\n","      <td>1</td>\n","      <td>wakeup</td>\n","      <td>10932.0</td>\n","      <td>2018-08-15T06:41:00-0400</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>038441c925bb</td>\n","      <td>2</td>\n","      <td>onset</td>\n","      <td>20244.0</td>\n","      <td>2018-08-15T19:37:00-0400</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>038441c925bb</td>\n","      <td>2</td>\n","      <td>wakeup</td>\n","      <td>27492.0</td>\n","      <td>2018-08-16T05:41:00-0400</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>038441c925bb</td>\n","      <td>3</td>\n","      <td>onset</td>\n","      <td>39996.0</td>\n","      <td>2018-08-16T23:03:00-0400</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      series_id  night   event     step                 timestamp\n","0  038441c925bb      1   onset   4992.0  2018-08-14T22:26:00-0400\n","1  038441c925bb      1  wakeup  10932.0  2018-08-15T06:41:00-0400\n","2  038441c925bb      2   onset  20244.0  2018-08-15T19:37:00-0400\n","3  038441c925bb      2  wakeup  27492.0  2018-08-16T05:41:00-0400\n","4  038441c925bb      3   onset  39996.0  2018-08-16T23:03:00-0400"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["array(['038441c925bb', '03d92c9f6f8a', '0402a003dae9', '04f547b8017d',\n","       '05e1944c3818', '062cae666e2a', '062dbd4c95e6', '08db4255286f',\n","       '0a96f4993bd7', '0cd1e3d0ed95', '0ce74d6d2106', '0cfc06c129cc',\n","       '0d0ad1e77851', '0dee4fda51c3', '0ec9fc461819', '0ef7d94fde99',\n","       '0f572d690310', '0f9e60a8e56d', '10469f6765bf', '1087d7b0ff2e',\n","       '10f8bc1f7b07', '12d01911d509', '1319a1935f48', '137771d19ca2',\n","       '137b99e936ab', '13b4d6a01d27', '148471991ffb', '154fe824ed87',\n","       '16fe2798ed0f', '1716cd4163b2', '1762ab70ec76', '188d4b7cd28b',\n","       '18a0ca03431d', '18b61dd5aae8', '1955d568d987', '1b92be89db4c',\n","       '1c7c0bad1263', '1d4569cbac0f', '1e6717d93c1d', '1f96b9668bdf',\n","       '207eded97727', '25e2b3dd9c3b', '2654a87be968', '27f09a6a858f',\n","       '280e08693c6d', '292a75c0b94e', '29c75c018220', '29d3469bd15d',\n","       '2b0a1fa8eba8', '2b8d87addea9', '2cd2340ca14d', '2e9ced2c7976',\n","       '2f7504d0f426', '2fbbee1a38e3', '2fc653ca75c7', '31011ade7c0a',\n","       '3318a0e3ed6f', '33ceeba8918a', '3452b878e596', '349c5562ee2c',\n","       '35826366dfc7', '361366da569e', '3664fe9233f9', '3665c86afaf5',\n","       '390b487231ce', '3a9a9dc2cbd9', '3aceb17ef7bd', '3be1545083b7',\n","       '3be2f86c3e45', '3c336d6ba566', '3d53bfea61d6', '3df0da2e5966',\n","       '405df1b41f9f', '40dce6018935', '416354edd92a', '449766346eb1',\n","       '44a41bba1ee7', '44d8c02b369e', '4743bdde25df', '483d6545417f',\n","       '4a31811f3558', '4ab54be1a403', '4ac356361be9', '4b45c36f8f5a',\n","       '4feda0596965', '519ae2d858b0', '51b23d177971', '51c49c540b4e',\n","       '51fdcc8d9fe7', '559ffb7c166a', '55a47ff9dc8a', '55b7f5c99930',\n","       '599ca4ed791b', '5aad18e7ce64', '5acc9d63b5fd', '5c088d7e916c',\n","       '5c55a5e717d6', '5e816f11f5c3', '5f40907ec171', '5f76965e10cf',\n","       '5f94bb3e1bed', '5ffd5e1e81ac', '601559e1777d', '60d31b0bec3b',\n","       '60e51cad2ffb', '612aa8ba44e2', '653622ac8363', '655f19eabf1e',\n","       '67f5fc60e494', '694faf956ebf', '6a4cd123bd69', '6bf95a3cf91c',\n","       '6ca4f4fca6a2', '6d6b9d22d48a', '6ee4ade1f2bd', '702bb5387b1e',\n","       '703b5efa9bc1', '72ba4a8afff4', '72bbd1ac3edf', '72d2234e84e4',\n","       '73fb772e50fb', '7476c0bd18d2', '7504165f497d', '752900afe3a6',\n","       '76237b9406d5', '77ca4db83644', '7822ee8fe3ec', '78569a801a38',\n","       '785c9ca4eff7', '7df249527c63', '7fd4284b7ee8', '804594bb1f06',\n","       '808652a666c6', '83fa182bec3a', '844f54dcab89', '854206f602d0',\n","       '87a6cbb7c4ed', '8877a6586606', '8898e6db816d', '89bd631d1769',\n","       '89c7daa72eee', '8a22387617c3', '8a306e0890c0', '8b159a98f485',\n","       '8b8b9e29171c', '8becc76ea607', '8e32047cbc1f', '8f6f15b9f598',\n","       '8fb18e36697d', '90eac42a9ec9', '91127c2b0e60', '91cb6c98201f',\n","       '9277be28a1cf', '927dd0c35dfd', '939932f1822d', '971207c6a525',\n","       '99237ce045e4', '99b829cbad2d', '9a340507e36a', '9aed9ee12ae2',\n","       '9b9cd7b7af8c', '9c91c546e095', '9ddd40f2cb36', '9ee455e4770d',\n","       '9fbdeffbe2ba', 'a167532acca2', 'a261bc4b7470', 'a2b0a64ec9cf',\n","       'a3e59c2ce3f6', 'a4e48102f402', 'a596ad0b82aa', 'a681f9b04b21',\n","       'a81f4472c637', 'a88088855de5', 'a9a2f7fac455', 'a9e5f5314bcb',\n","       'aa81faa78747', 'ad425f3ee76d', 'aed3850f65f0', 'af91d9a50547',\n","       'b1831c4979da', 'b364205aba43', 'b4b75225b224', 'b7188813d58a',\n","       'b737f8c78ec5', 'b750c8c1556c', 'b7fc34995d0f', 'b84960841a75',\n","       'ba8083a2c3b8', 'bb5612895813', 'bccf2f2819f8', 'bdfce9ce62b9',\n","       'bf00506437aa', 'bfa54bd26187', 'bfe41e96d12f', 'c107b5789660',\n","       'c289c8a823e0', 'c3072a759efb', 'c38707ef76df', 'c535634d7dcd',\n","       'c5365a55ebb7', 'c5d08fc3e040', 'c6788e579967', 'c68260cc9e8f',\n","       'c75b4b207bea', 'c7b1283bb7eb', 'c7b2155a4a47', 'c7d693f24684',\n","       'c8053490cec2', 'c908a0ad3e31', 'ca730dbf521d', 'ca732a3c37f7',\n","       'cca14d1966c1', 'ccdee561ee5d', 'ce85771a714c', 'ce9164297046',\n","       'cf13ed7e457a', 'cfeb11428dd7', 'd043c0ca71cd', 'd0f613c700f7',\n","       'd150801f3145', 'd25e479ecbb7', 'd2d6b9af0553', 'd2fef7e4defd',\n","       'd3dddd3c0e00', 'd515236bdeec', 'd5be621fd9aa', 'd5e47b94477e',\n","       'd8de352c2657', 'd93b0c7de16b', 'd9e887091a5c', 'dacc6d652e35',\n","       'db5e0ee1c0ab', 'db75092f0530', 'dc80ca623d71', 'de6fedfb6139',\n","       'def21f50dd3c', 'df33ae359fb5', 'dfc3ccebfdc9', 'dff367373725',\n","       'e0686434d029', 'e0d7b0dcf9f3', 'e11b9d69f856', 'e1f2a4f991cb',\n","       'e1f5abb82285', 'e2a849d283c0', 'e2b60820c325', 'e30cb792a2bc',\n","       'e34b496b84ce', 'e4500e7e19e1', 'e586cbfa7762', 'e69aff66e0cb',\n","       'e6ddbaaf0639', 'e867b5133665', 'e8d0a37c3eba', 'ea0770830757',\n","       'ebb6fae8ed43', 'ebd76e93ec7d', 'ece2561f07e9', 'ee4e0e3afd3d',\n","       'eec197a4bdca', 'eef041dd50aa', 'efbfc4526d58', 'f0482490923c',\n","       'f2c2436cf7b7', 'f564985ab692', 'f56824b503a0', 'f6d2cc003183',\n","       'f7eb179216c2', 'f88e18cb4100', 'f8a8da8bdd00', 'f981a0805fd0',\n","       'fa149c3c4bde', 'fb223ed2278c', 'fbf33b1a2c10', 'fcca183903b7',\n","       'fe90110788d2'], dtype=object)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['038441c925bb' '03d92c9f6f8a' '0402a003dae9' '04f547b8017d'\n"," '05e1944c3818' '062cae666e2a' '08db4255286f' '0a96f4993bd7'\n"," '0cfc06c129cc' '0d0ad1e77851' '0dee4fda51c3' '0ec9fc461819'\n"," '0f572d690310' '0f9e60a8e56d' '10469f6765bf' '10f8bc1f7b07'\n"," '12d01911d509' '137771d19ca2' '148471991ffb' '154fe824ed87'\n"," '16fe2798ed0f' '1716cd4163b2' '188d4b7cd28b' '18a0ca03431d'\n"," '18b61dd5aae8' '1955d568d987' '1b92be89db4c' '1c7c0bad1263'\n"," '1d4569cbac0f' '1e6717d93c1d' '1f96b9668bdf' '207eded97727'\n"," '25e2b3dd9c3b' '27f09a6a858f' '280e08693c6d' '29d3469bd15d'\n"," '2b0a1fa8eba8' '2b8d87addea9' '2cd2340ca14d' '2e9ced2c7976'\n"," '2f7504d0f426' '2fbbee1a38e3' '2fc653ca75c7' '31011ade7c0a'\n"," '3318a0e3ed6f' '33ceeba8918a' '3452b878e596' '349c5562ee2c'\n"," '361366da569e' '3664fe9233f9' '3665c86afaf5' '390b487231ce'\n"," '3a9a9dc2cbd9' '3be1545083b7' '3c336d6ba566' '3d53bfea61d6'\n"," '3df0da2e5966' '405df1b41f9f' '416354edd92a' '44a41bba1ee7'\n"," '44d8c02b369e' '4743bdde25df' '4a31811f3558' '4ab54be1a403'\n"," '4b45c36f8f5a' '519ae2d858b0' '51b23d177971' '51c49c540b4e'\n"," '51fdcc8d9fe7' '559ffb7c166a' '55b7f5c99930' '599ca4ed791b'\n"," '5aad18e7ce64' '5acc9d63b5fd' '5c088d7e916c' '5c55a5e717d6'\n"," '5f40907ec171' '5f76965e10cf' '5f94bb3e1bed' '601559e1777d'\n"," '60d31b0bec3b' '60e51cad2ffb' '612aa8ba44e2' '653622ac8363'\n"," '655f19eabf1e' '67f5fc60e494' '694faf956ebf' '6a4cd123bd69'\n"," '6bf95a3cf91c' '6ca4f4fca6a2' '6d6b9d22d48a' '702bb5387b1e'\n"," '703b5efa9bc1' '72ba4a8afff4' '72d2234e84e4' '73fb772e50fb'\n"," '7476c0bd18d2' '7504165f497d' '752900afe3a6' '76237b9406d5'\n"," '78569a801a38' '785c9ca4eff7' '7df249527c63' '7fd4284b7ee8'\n"," '804594bb1f06' '808652a666c6' '83fa182bec3a' '844f54dcab89'\n"," '854206f602d0' '87a6cbb7c4ed' '8877a6586606' '8898e6db816d'\n"," '89bd631d1769' '89c7daa72eee' '8a22387617c3' '8b159a98f485'\n"," '8becc76ea607' '8e32047cbc1f' '8f6f15b9f598' '8fb18e36697d'\n"," '90eac42a9ec9' '91127c2b0e60' '91cb6c98201f' '927dd0c35dfd'\n"," '939932f1822d' '99237ce045e4' '99b829cbad2d' '9a340507e36a'\n"," '9b9cd7b7af8c' '9c91c546e095' '9ddd40f2cb36' '9ee455e4770d'\n"," '9fbdeffbe2ba' 'a261bc4b7470' 'a3e59c2ce3f6' 'a4e48102f402'\n"," 'a596ad0b82aa' 'a681f9b04b21' 'a81f4472c637' 'a88088855de5'\n"," 'a9a2f7fac455' 'a9e5f5314bcb' 'ad425f3ee76d' 'aed3850f65f0'\n"," 'af91d9a50547' 'b1831c4979da' 'b4b75225b224' 'b7188813d58a'\n"," 'b737f8c78ec5' 'b750c8c1556c' 'b7fc34995d0f' 'b84960841a75'\n"," 'ba8083a2c3b8' 'bb5612895813' 'bccf2f2819f8' 'bdfce9ce62b9'\n"," 'bf00506437aa' 'bfe41e96d12f' 'c107b5789660' 'c289c8a823e0'\n"," 'c3072a759efb' 'c38707ef76df' 'c535634d7dcd' 'c5365a55ebb7'\n"," 'c5d08fc3e040' 'c68260cc9e8f' 'c75b4b207bea' 'c7b1283bb7eb'\n"," 'c7b2155a4a47' 'c7d693f24684' 'c908a0ad3e31' 'ca730dbf521d'\n"," 'ca732a3c37f7' 'ce85771a714c' 'cf13ed7e457a' 'cfeb11428dd7'\n"," 'd043c0ca71cd' 'd0f613c700f7' 'd25e479ecbb7' 'd2d6b9af0553'\n"," 'd2fef7e4defd' 'd3dddd3c0e00' 'd5be621fd9aa' 'd5e47b94477e'\n"," 'd8de352c2657' 'd93b0c7de16b' 'dacc6d652e35' 'db5e0ee1c0ab'\n"," 'dc80ca623d71' 'de6fedfb6139' 'df33ae359fb5' 'dfc3ccebfdc9'\n"," 'dff367373725' 'e0686434d029' 'e11b9d69f856' 'e1f2a4f991cb'\n"," 'e1f5abb82285' 'e2a849d283c0' 'e2b60820c325' 'e30cb792a2bc'\n"," 'e34b496b84ce' 'e4500e7e19e1' 'e586cbfa7762' 'e69aff66e0cb'\n"," 'e6ddbaaf0639' 'e867b5133665' 'e8d0a37c3eba' 'ebd76e93ec7d'\n"," 'ee4e0e3afd3d' 'eef041dd50aa' 'f0482490923c' 'f2c2436cf7b7'\n"," 'f564985ab692' 'f56824b503a0' 'f6d2cc003183' 'f7eb179216c2'\n"," 'f8a8da8bdd00' 'f981a0805fd0' 'fb223ed2278c' 'fcca183903b7'\n"," 'fe90110788d2'] ['062dbd4c95e6' '0cd1e3d0ed95' '0ce74d6d2106' '0ef7d94fde99'\n"," '1087d7b0ff2e' '1319a1935f48' '137b99e936ab' '13b4d6a01d27'\n"," '1762ab70ec76' '2654a87be968' '292a75c0b94e' '29c75c018220'\n"," '35826366dfc7' '3aceb17ef7bd' '3be2f86c3e45' '40dce6018935'\n"," '449766346eb1' '483d6545417f' '4ac356361be9' '4feda0596965'\n"," '55a47ff9dc8a' '5e816f11f5c3' '5ffd5e1e81ac' '6ee4ade1f2bd'\n"," '72bbd1ac3edf' '77ca4db83644' '7822ee8fe3ec' '8a306e0890c0'\n"," '8b8b9e29171c' '9277be28a1cf' '971207c6a525' '9aed9ee12ae2'\n"," 'a167532acca2' 'a2b0a64ec9cf' 'aa81faa78747' 'b364205aba43'\n"," 'bfa54bd26187' 'c6788e579967' 'c8053490cec2' 'cca14d1966c1'\n"," 'ccdee561ee5d' 'ce9164297046' 'd150801f3145' 'd515236bdeec'\n"," 'd9e887091a5c' 'db75092f0530' 'def21f50dd3c' 'e0d7b0dcf9f3'\n"," 'ea0770830757' 'ebb6fae8ed43' 'ece2561f07e9' 'eec197a4bdca'\n"," 'efbfc4526d58' 'f88e18cb4100' 'fa149c3c4bde' 'fbf33b1a2c10']\n"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","from plotly.offline import init_notebook_mode\n","init_notebook_mode()\n","from IPython.display import Markdown\n","\n","import dateutil.relativedelta as rd\n","from plotly import __version__\n","from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n","from plotly.graph_objs import *\n","from plotly.tools import FigureFactory as FF\n","\n","import dateutil.relativedelta as rd\n","from scipy.interpolate import interp1d\n","\n","from math import pi, sqrt, exp\n","import sklearn,sklearn.model_selection\n","import torch\n","from torch import nn,Tensor\n","import random\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n","from sklearn.metrics import average_precision_score\n","from timm.scheduler import CosineLRScheduler\n","\n","import pandas as pd\n","import numpy as np\n","import gc\n","import math\n","import matplotlib.pyplot as plt\n","plt.style.use(\"ggplot\")\n","\n","from tqdm.auto import tqdm\n","\n","from pyarrow.parquet import ParquetFile\n","import pyarrow as pa \n","\n","import ctypes\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","import copy\n","import event_detection_ap as mapmetric"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-11-02T14:25:33.207514Z","iopub.status.busy":"2023-11-02T14:25:33.207117Z","iopub.status.idle":"2023-11-02T14:25:40.373631Z","shell.execute_reply":"2023-11-02T14:25:40.372613Z","shell.execute_reply.started":"2023-11-02T14:25:33.207476Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/siavash/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["\n","\n","mapmetric.series_id_column_name = 'series_id'\n","mapmetric.time_column_name = 'step'\n","mapmetric.event_column_name = 'event'\n","mapmetric.score_column_name = 'score'\n","mapmetric.use_scoring_intervals = False\n","tolerance_intervals = [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]\n","tolerances = {'wakeup': tolerance_intervals, 'onset': tolerance_intervals}\n","\n","class PATHS:\n","    MAIN_DIR = \"/home/siavash/random/data/\"\n","    SPLIT_DIR = \"/home/siavash/random/grunet/data/\"\n","    \n","    # CSV FILES : \n","    SUBMISSION = MAIN_DIR + \"sample_submission.csv\"\n","    TRAIN_EVENTS = MAIN_DIR + \"train_events.csv\"\n","    # PARQUET FILES:\n","    TRAIN_SERIES = MAIN_DIR + \"train_series.parquet\"\n","    TEST_SERIES = MAIN_DIR + \"test_series.parquet\"\n","    \n","    @staticmethod\n","    def get_series_filename(series_id):\n","        f = f'{series_id}_test_series.parquet'\n","        return PATHS.SPLIT_DIR + f\n","    \n","class CFG:\n","    DEMO_MODE = False\n","    VERBOSE = True\n","    \n","    SEED = 42\n","    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:25:40.375154Z","iopub.status.busy":"2023-11-02T14:25:40.374719Z","iopub.status.idle":"2023-11-02T14:25:40.38465Z","shell.execute_reply":"2023-11-02T14:25:40.383786Z","shell.execute_reply.started":"2023-11-02T14:25:40.375127Z"},"trusted":true},"outputs":[],"source":["def torch_fix_seed(seed=42):\n","    # Python random\n","    random.seed(seed)\n","    # Numpy\n","    np.random.seed(seed)\n","    # Pytorch\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    # torch.backends.cudnn.deterministic = True\n","    # torch.use_deterministic_algorithms = True\n","    # torch.backends.cudnn.benchmark = True\n","\n","torch_fix_seed(CFG.SEED)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:25:40.387167Z","iopub.status.busy":"2023-11-02T14:25:40.386787Z","iopub.status.idle":"2023-11-02T14:25:40.39268Z","shell.execute_reply":"2023-11-02T14:25:40.391799Z","shell.execute_reply.started":"2023-11-02T14:25:40.387143Z"},"trusted":true},"outputs":[],"source":["def clean_memory():\n","    gc.collect()\n","    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:25:40.393883Z","iopub.status.busy":"2023-11-02T14:25:40.393638Z","iopub.status.idle":"2023-11-02T14:25:40.438153Z","shell.execute_reply":"2023-11-02T14:25:40.43731Z","shell.execute_reply.started":"2023-11-02T14:25:40.393862Z"},"trusted":true},"outputs":[{"data":{"text/plain":["14508"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train_events = pd.read_csv(\"/home/siavash/random/data/train_events.csv\")\n","len(train_events)"]},{"cell_type":"markdown","metadata":{},"source":["First step is to rid ourselves of sequences that have no events, or sequences that have a few events."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:25:40.439911Z","iopub.status.busy":"2023-11-02T14:25:40.439344Z","iopub.status.idle":"2023-11-02T14:25:40.447118Z","shell.execute_reply":"2023-11-02T14:25:40.446258Z","shell.execute_reply.started":"2023-11-02T14:25:40.439879Z"},"trusted":true},"outputs":[],"source":["def get_longest_continuous(gdf):\n","    \"\"\"Function will find longest subsequence of a training set where there is an event each night\"\"\"\n","    c = sorted(list(set(gdf[gdf['event'] == 'onset'].dropna().night.unique()) & set(gdf[gdf['event'] == 'wakeup'].dropna().night.unique())))\n","    start = -1\n","    end = -1\n","    m = 0\n","    save = 0,0\n","    last = start\n","    for x in c:\n","        if x == last + 1:\n","            end = x\n","            last = x\n","        else:\n","            v = end - start\n","            if v > m:\n","                save = start, end\n","                m = v\n","            start = x\n","            end = x\n","            last = x\n","\n","    v = end - start\n","    if v > m:\n","        save = start, end\n","        m = v\n","\n","#     print(f'Max length is {m} from {save}')\n","#     print(c)\n","    return save"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:25:40.44861Z","iopub.status.busy":"2023-11-02T14:25:40.448277Z","iopub.status.idle":"2023-11-02T14:25:41.228581Z","shell.execute_reply":"2023-11-02T14:25:41.227645Z","shell.execute_reply.started":"2023-11-02T14:25:40.44858Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Drop 18\n"]}],"source":["drop_series = []\n","continuous = {}\n","for series_id, gdf in train_events.groupby('series_id'):\n","    tmp = gdf.dropna()\n","    # print(series_id, len(tmp))\n","    if len(tmp) == 0:\n","        drop_series.append(series_id)\n","    else:\n","        start, end = get_longest_continuous(gdf)\n","        if end - start == 0:\n","            drop_series.append(series_id)\n","        else:\n","            continuous[series_id] = start, end\n","        \n","print(f'Drop {len(drop_series)}')"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:25:41.230252Z","iopub.status.busy":"2023-11-02T14:25:41.229851Z","iopub.status.idle":"2023-11-02T14:25:41.23883Z","shell.execute_reply":"2023-11-02T14:25:41.237932Z","shell.execute_reply.started":"2023-11-02T14:25:41.230213Z"},"trusted":true},"outputs":[{"data":{"text/plain":["277"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["series_ids = train_events.series_id.unique()\n","len(series_ids)"]},{"cell_type":"markdown","metadata":{},"source":["## Utility Functions"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:25:41.24038Z","iopub.status.busy":"2023-11-02T14:25:41.240063Z","iopub.status.idle":"2023-11-02T14:25:41.250303Z","shell.execute_reply":"2023-11-02T14:25:41.249357Z","shell.execute_reply.started":"2023-11-02T14:25:41.240328Z"},"trusted":true},"outputs":[],"source":["def compare_predictions(valid_ds, i, net):\n","    \"\"\"Utility function to return dataframes of predicted and actual events with their distributions\"\"\"\n","    net.eval()\n","    with torch.no_grad():\n","        X, Y = valid_ds[i]\n","        Y = Y.to(CFG.DEVICE, non_blocking=True)\n","        pred = torch.zeros(Y.shape).to(CFG.DEVICE, non_blocking=True)\n","\n","        h = None\n","\n","        seq_len = X.shape[0]\n","        for j in range(0, seq_len, max_chunk_size):\n","            X_chunk = X[j: j + max_chunk_size].float().to(CFG.DEVICE, non_blocking=True)\n","            y_pred, h = net(X_chunk, h)\n","            h = [hi.detach() for hi in h]\n","            pred[j: j+max_chunk_size, :] = y_pred\n","\n","            del X_chunk, y_pred\n","        clean_memory()\n","    res_df = pd.DataFrame(torch.softmax(pred.cpu(), axis=0).numpy(), columns=['wakeup_val', 'onset_val'])   \n","    act_df = pd.DataFrame(Y.cpu().numpy(), columns=['wakeup_val', 'onset_val'])\n","    return res_df, act_df"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:25:41.253651Z","iopub.status.busy":"2023-11-02T14:25:41.253411Z","iopub.status.idle":"2023-11-02T14:25:41.262263Z","shell.execute_reply":"2023-11-02T14:25:41.261378Z","shell.execute_reply.started":"2023-11-02T14:25:41.25363Z"},"trusted":true},"outputs":[],"source":["def get_predictions(res_df, target, SIGMA):\n","    \"\"\"Function will take a predicted dataframe, and find local maxima to get event location.  The score is determined by the area under the curve\"\"\"\n","    q = res_df[target].max() * 0.1\n","    tmp = res_df.loc[res_df[target] > q].copy()\n","    # print(f'Target max = {q}, len = {len(tmp)}')\n","    tmp['gap'] = tmp['step'].diff()\n","    tmp = tmp[tmp['gap'] > 5*5]\n","    # print(f'Target max = {q}, len = {len(tmp)}')\n","    res = []\n","    for i in range(len(tmp) + 1):\n","        start_i = 0 if i == 0 else tmp['step'].iloc[i-1]\n","        end_i = tmp['step'].iloc[i] if i < len(tmp) else res_df['step'].max()\n","        v = res_df.loc[(res_df['step'] > start_i) & (res_df['step'] < end_i)]\n","        if v[target].max() > q:\n","            # print('Locate in ', start_i, end_i)\n","            idx = v.idxmax()[target]\n","            step = v.loc[idx, 'step']\n","            span = 3*SIGMA\n","            score = res_df.loc[(res_df['step'] > step - span) & (res_df['step'] < step + span), target].sum()\n","            res.append([step, target, score])\n","            \n","    return res"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:25:41.263707Z","iopub.status.busy":"2023-11-02T14:25:41.26338Z","iopub.status.idle":"2023-11-02T14:25:41.275853Z","shell.execute_reply":"2023-11-02T14:25:41.275066Z","shell.execute_reply.started":"2023-11-02T14:25:41.263684Z"},"trusted":true},"outputs":[],"source":["def create_predictions(test_ds, i, net):\n","    \"\"\"Function to create the prediction dataframe\"\"\"\n","    net.eval()\n","    with torch.no_grad():\n","        X = test_ds[i]\n","        pred = torch.zeros(X.shape).to(CFG.DEVICE, non_blocking=True)\n","\n","        h = None\n","\n","        seq_len = X.shape[0]\n","        for j in range(0, seq_len, max_chunk_size):\n","            X_chunk = X[j: j + max_chunk_size].float().to(CFG.DEVICE, non_blocking=True)\n","            y_pred, h = net(X_chunk, h)\n","            h = [hi.detach() for hi in h]\n","            pred[j: j+max_chunk_size, :] = y_pred\n","\n","            del X_chunk, y_pred\n","        clean_memory()\n","    res_df = pd.DataFrame(torch.softmax(pred.cpu(), axis=0).numpy(), columns=['wakeup_val', 'onset_val'])   \n","    return res_df"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:25:41.277256Z","iopub.status.busy":"2023-11-02T14:25:41.276932Z","iopub.status.idle":"2023-11-02T14:25:41.292026Z","shell.execute_reply":"2023-11-02T14:25:41.291127Z","shell.execute_reply.started":"2023-11-02T14:25:41.277226Z"},"trusted":true},"outputs":[],"source":["class SleepDatasetTrain(Dataset):\n","    \"\"\"\n","    Dataset for Child Mind Sleep States.  We have it output just the raw anglez and enmo variables.\n","    \n","    :param series_ids: list of series ids in this set\n","    :param events: The events dataframe\n","    :param len_mult: The total length of the sequence must be a multiple of this integer\n","    :param continuous: dictionary of series_id to start and end points, if we want to trim to continuous series\n","    :param sigma: The width of the distribution to use for output.\n","    \n","    \"\"\"\n","    def __init__(\n","        self,\n","        series_ids,\n","        events,\n","        len_mult,\n","        continuous = None,\n","        sigma = None\n","    ):\n","        self.series_ids = series_ids\n","        self.continuous = continuous\n","        self.len_mult = len_mult\n","        if events is not None:\n","            self.events = events\n","            self.sigma = sigma\n","        else:\n","            self.events = None\n","            self.sigma = None\n","    \n","    def load_data(self, series_id):\n","        filename = PATHS.get_series_filename(series_id)\n","        data = pd.read_parquet(filename)\n","        if self.events is not None:\n","            if self.continuous is not None:\n","                start, end = self.continuous[series_id]\n","            else:\n","                start, end = 0, 1000000\n","            gap = 6*60*12\n","            tmp = self.events[(self.events.series_id == series_id) & (self.events.night >= start) & (self.events.night <= end)]\n","            data = data[(data.step > (tmp.step.min() - gap)) & (data.step < (tmp.step.max() + gap))]\n","            \n","            data = data.set_index(['series_id', 'step']).join(tmp.set_index(['series_id', 'step'])[['event', 'night']]).reset_index()\n","            norm = 1/ np.sqrt(pi / self.sigma)\n","            for evt in ['wakeup', 'onset']:\n","                steps = data[data.event == evt]['step'].values\n","                col = f'{evt}_val'\n","                data[col] = 0.0\n","                for i in steps:\n","                    x = 0.5*((data.step.astype(np.int64) - i)/self.sigma)**2\n","                    data[col] += np.exp(-x)*norm\n","                data[col] /= data[col].sum()\n","                \n","        n = int((len(data) // len_mult) * len_mult)\n","        \n","        return data.iloc[:n]\n","        \n","    def __len__(self):\n","        return len(self.series_ids)\n","\n","    def __getitem__(self, index):\n","        series_id = self.series_ids[index]\n","        data = self.load_data(series_id)\n","        X = data[['anglez','enmo']].values.astype(np.float32)\n","        X = torch.from_numpy(X)\n","        if self.sigma is not None:\n","            Y = data[['wakeup_val', 'onset_val']].values.astype(np.float32)\n","            Y = torch.from_numpy(Y)\n","            return X, Y\n","        else:\n","            return X"]},{"cell_type":"markdown","metadata":{},"source":["## Network\n","\n","I decided that we are detecting edge points, similar to segmentation of images, the a UNET like architecture would be helpful.  Instead of downsampling to get indicators, we let the network learn the convolution.  Since we have wakeup events always after onset events, there is some time component, which is why the GRU cells are in the bottleneck of the UNET.\n","\n","\n","Possible improvements:\n","\n","1. The encoder layers and decoder layers could better include more convolutions\n","2. We can add skip connections.\n","3. We could just put more indicators in the input.  \n","4. We can add a time component to the input.\n","5. We can increase the size of the hidden layers.\n","\n","\n","The Residual GRU is from \n","\n","https://www.kaggle.com/competitions/tlvmc-parkinsons-freezing-gait-prediction/discussion/416410"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:25:41.293266Z","iopub.status.busy":"2023-11-02T14:25:41.293004Z","iopub.status.idle":"2023-11-02T14:25:41.312227Z","shell.execute_reply":"2023-11-02T14:25:41.311237Z","shell.execute_reply.started":"2023-11-02T14:25:41.293245Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:25:41.31383Z","iopub.status.busy":"2023-11-02T14:25:41.313504Z","iopub.status.idle":"2023-11-02T14:25:41.327387Z","shell.execute_reply":"2023-11-02T14:25:41.326376Z","shell.execute_reply.started":"2023-11-02T14:25:41.313802Z"},"trusted":true},"outputs":[],"source":["class ResidualBiGRU(nn.Module):\n","    def __init__(self, hidden_size, n_layers=1, bidir=True):\n","        super(ResidualBiGRU, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.n_layers = n_layers\n","\n","        self.gru = nn.GRU(\n","            hidden_size,\n","            hidden_size,\n","            n_layers,\n","            batch_first=True,\n","            bidirectional=bidir,\n","        )\n","        dir_factor = 2 if bidir else 1\n","        self.fc1 = nn.Linear(\n","            hidden_size * dir_factor, hidden_size * dir_factor * 2\n","        )\n","        self.ln1 = nn.LayerNorm(hidden_size * dir_factor * 2)\n","        self.fc2 = nn.Linear(hidden_size * dir_factor * 2, hidden_size)\n","        self.ln2 = nn.LayerNorm(hidden_size)\n","\n","    def forward(self, x, h=None):\n","        res, new_h = self.gru(x, h)\n","        # res.shape = (batch_size, sequence_size, 2*hidden_size)\n","\n","        res = self.fc1(res)\n","        res = self.ln1(res)\n","        res = nn.functional.relu(res)\n","\n","        res = self.fc2(res)\n","        res = self.ln2(res)\n","        res = nn.functional.relu(res)\n","\n","        # skip connection\n","        res = res + x\n","\n","        return res, new_h\n","\n","class MultiResidualBiGRU(nn.Module):\n","    def __init__(self, input_size, hidden_size, out_size, n_layers, bidir=True):\n","        super(MultiResidualBiGRU, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.out_size = out_size\n","        self.n_layers = n_layers\n","\n","        self.fc_in = nn.Linear(input_size, hidden_size)\n","        self.ln = nn.LayerNorm(hidden_size)\n","        self.res_bigrus = nn.ModuleList(\n","            [\n","                ResidualBiGRU(hidden_size, n_layers=1, bidir=bidir)\n","                for _ in range(n_layers)\n","            ]\n","        )\n","        self.fc_out = nn.Linear(hidden_size, out_size)\n","\n","    def forward(self, x, h=None):\n","        # if we are at the beginning of a sequence (no hidden state)\n","        if h is None:\n","            # (re)initialize the hidden state\n","            h = [None for _ in range(self.n_layers)]\n","\n","        x = self.fc_in(x)\n","        x = self.ln(x)\n","        x = nn.functional.relu(x)\n","\n","        new_h = []\n","        for i, res_bigru in enumerate(self.res_bigrus):\n","            x, new_hi = res_bigru(x, h[i])\n","            new_h.append(new_hi)\n","\n","        x = self.fc_out(x)\n","#         x = F.normalize(x,dim=0)\n","        return x, new_h  # log probabilities + hidden states\n"]},{"cell_type":"markdown","metadata":{},"source":["I have elected to downsample using stride, but you could instead do it via some pooling method."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["class Wave_Block(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels, dilation_rates, kernel_size):\n","        super(Wave_Block, self).__init__()\n","        self.num_rates = dilation_rates\n","        self.convs = nn.ModuleList()\n","        self.filter_convs = nn.ModuleList()\n","        self.gate_convs = nn.ModuleList()\n","\n","        self.convs.append(nn.Conv1d(in_channels, out_channels, kernel_size=1))\n","        dilation_rates = [2 ** i for i in range(dilation_rates)]\n","        for dilation_rate in dilation_rates:\n","            self.filter_convs.append(\n","                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=int((dilation_rate*(kernel_size-1))/2), dilation=dilation_rate))\n","            self.gate_convs.append(\n","                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=int((dilation_rate*(kernel_size-1))/2), dilation=dilation_rate))\n","            self.convs.append(nn.Conv1d(out_channels, out_channels, kernel_size=1))\n","\n","    def forward(self, x):\n","        x = self.convs[0](x)\n","        res = x\n","        for i in range(self.num_rates):\n","            x = torch.tanh(self.filter_convs[i](x)) * torch.sigmoid(self.gate_convs[i](x))\n","            x = self.convs[i + 1](x)\n","            res = res + x\n","        return res"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:25:41.329355Z","iopub.status.busy":"2023-11-02T14:25:41.328608Z","iopub.status.idle":"2023-11-02T14:25:41.341302Z","shell.execute_reply":"2023-11-02T14:25:41.3406Z","shell.execute_reply.started":"2023-11-02T14:25:41.329306Z"},"trusted":true},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    def __init__(self, in_channels, hidden_size, kernel_size, dilation, use_layernorm, print_shape):\n","        super(EncoderLayer, self).__init__()\n","        self.wave_block = Wave_Block(in_channels, hidden_size, dilation, kernel_size)\n","        self.ln = nn.LayerNorm(hidden_size) if use_layernorm else None\n","        self.print_shape = print_shape\n","        \n","    def forward(self, x):\n","        x = self.wave_block(x.transpose(-1,-2))\n","        if self.print_shape:\n","            print('After Conv', x.shape)\n","        if self.ln is not None:\n","            x = self.ln(x.transpose(-1, -2))\n","        else:\n","            x = x.transpose(-1,-2)\n","        if self.print_shape:\n","            print('After Layernorm', x.shape)\n","        x = nn.functional.relu(x)\n","        return x"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:25:41.342576Z","iopub.status.busy":"2023-11-02T14:25:41.342302Z","iopub.status.idle":"2023-11-02T14:25:41.357378Z","shell.execute_reply":"2023-11-02T14:25:41.356519Z","shell.execute_reply.started":"2023-11-02T14:25:41.342554Z"},"trusted":true},"outputs":[],"source":["class GRUNET(nn.Module):\n","    def __init__(\n","        self,\n","        arch,\n","        out_channels,\n","        kernel_size,\n","        stride,\n","        dconv_padding,\n","        hidden_size,\n","        n_layers,\n","        bidir=True,\n","        print_shape=False,\n","    ):\n","        super(GRUNET, self).__init__()\n","\n","        self.input_size = in_channels\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","        self.hidden_size = hidden_size\n","        # self.out_size = out_size\n","        self.n_layers = n_layers\n","        self.padding = kernel_size // 2\n","        self.print_shape = print_shape\n","        self.arch = arch\n","        self.dilation = 1\n","        assert arch[-1][1] == hidden_size\n","\n","        self.conv = nn.Sequential(\n","            *[\n","                EncoderLayer(\n","                    in_chan,\n","                    out_chan,\n","                    ksize,\n","                    dilation=dil,\n","                    use_layernorm=True,\n","                    print_shape=print_shape,\n","                )\n","                for in_chan, out_chan, stride, ksize, dil in self.arch\n","            ]\n","        )\n","        self.late_conv = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=8, stride=8, padding=0)\n","        self.res_bigrus = nn.ModuleList(\n","            [\n","                ResidualBiGRU(hidden_size, n_layers=1, bidir=bidir)\n","                for _ in range(n_layers)\n","            ]\n","        )\n","        self.dconv = nn.Sequential(\n","            *sum(\n","                [\n","                    [\n","                        nn.ConvTranspose1d(\n","                            out_chan,\n","                            in_chan,\n","                            ksize,\n","                            stride=stride,\n","                            padding=ksize // 2,\n","                            dilation=self.dilation,\n","                            output_padding=1,\n","                        ),\n","                        nn.Conv1d(\n","                            in_chan,\n","                            in_chan,\n","                            ksize,\n","                            stride=1,\n","                            padding=ksize // 2,\n","                            dilation=self.dilation,\n","                        ),\n","                        nn.ReLU(),\n","                        nn.Conv1d(\n","                            in_chan,\n","                            in_chan,\n","                            ksize,\n","                            stride=1,\n","                            padding=ksize // 2,\n","                            dilation=self.dilation,\n","                        ),\n","                        nn.ReLU(),\n","                    ]\n","                    for in_chan, out_chan, stride, ksize, dil in reversed(arch)\n","                ],\n","                [],\n","            )\n","        )\n","        self.output_layer = nn.Conv1d(2, 2, kernel_size=1, stride=1)\n","        self.avgpool = nn.AvgPool1d(8)\n","\n","    def forward(self, x, h=None):\n","        # if we are at the beginning of a sequence (no hidden state)\n","        init_shape = x.shape\n","        if h is None:\n","            # (re)initialize the hidden state\n","            h = [None for _ in range(self.n_layers)]\n","\n","        if self.print_shape:\n","            print(\"In\", x.shape)\n","        x = self.conv(x)\n","        if self.print_shape:\n","            print(\"After EncoderLayer\", x.shape)\n","        new_h = []\n","        for i, res_bigru in enumerate(self.res_bigrus):\n","            x, new_hi = res_bigru(x, h[i])\n","            new_h.append(new_hi)\n","        if self.print_shape:\n","            print(\"After GRU\", x.shape)\n","        x = self.dconv(x.transpose(-1, -2))\n","        if self.print_shape:\n","            print(\"After DConv\", x.shape)\n","\n","        x = self.output_layer(x)\n","        if self.print_shape:\n","            print(\"After SmoothConv\", x.shape)\n","        x = self.avgpool(x)\n","        x = x.transpose(-1, -2)\n","\n","\n","\n","        if self.print_shape:\n","            print(\"After AvgPool\", x.shape)\n","        return x, new_h  # probabilities + hidden states"]},{"cell_type":"markdown","metadata":{},"source":["Specify and architecture, and make sure that the network returns a prediction that matches the shape of the input.  Really we just want it for length."]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:25:41.358832Z","iopub.status.busy":"2023-11-02T14:25:41.358567Z","iopub.status.idle":"2023-11-02T14:25:41.691537Z","shell.execute_reply":"2023-11-02T14:25:41.690684Z","shell.execute_reply.started":"2023-11-02T14:25:41.358809Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["In torch.Size([1040, 2])\n","After Conv torch.Size([8, 1040])\n","After Layernorm torch.Size([1040, 8])\n","After Conv torch.Size([32, 1040])\n","After Layernorm torch.Size([1040, 32])\n","After Conv torch.Size([64, 1040])\n","After Layernorm torch.Size([1040, 64])\n","After EncoderLayer torch.Size([1040, 64])\n"]},{"name":"stdout","output_type":"stream","text":["After GRU torch.Size([1040, 64])\n","After DConv torch.Size([2, 8320])\n","After SmoothConv torch.Size([2, 8320])\n","After AvgPool torch.Size([1040, 2])\n"]}],"source":["arch = [(2, 8,  2, 17, 4),\n","        (8, 32,  2, 11, 2),\n","        (32, 64, 2, 7, 1)]\n","\n","in_channels = 2\n","hidden_size = arch[-1][1]\n","kernel_size=25\n","stride = arch[-1][0]\n","dilation = 1\n","n_layers = 5\n","dconv_padding=5\n","len_mult = 2**len(arch)\n","\n","\n","net = GRUNET(arch=arch,out_channels=2, hidden_size=hidden_size, kernel_size=kernel_size, stride=stride, dconv_padding=dconv_padding, n_layers=n_layers, bidir=True, print_shape=True)\n","X = torch.randn(1040, 2).float()\n","Z, h = net(X)\n","assert X.shape == Z.shape\n"]},{"cell_type":"markdown","metadata":{},"source":["## Training\n","\n","We sum the loss over the entire series in order to do our loss and step.  This means a batch is each series, but they have variable length inside the batch since series have different sizes.  \n","\n","In choosing the width, we start with a wide width because it helps the model converge, then tighten it at later epochs.  You can change this!  \n","\n","1. I have chosen KLDivLoss, but this might not be the best.  Experiment with the different loss functions!\n","2. Experiment with learning rates and schedules.  Maybe it is better to restart the learning rate when we change sigma!\n","3. Use center of mass instead of maximum to predict the step!"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:25:41.694222Z","iopub.status.busy":"2023-11-02T14:25:41.692582Z","iopub.status.idle":"2023-11-02T14:25:41.700463Z","shell.execute_reply":"2023-11-02T14:25:41.69966Z","shell.execute_reply.started":"2023-11-02T14:25:41.694196Z"},"trusted":true},"outputs":[],"source":["max_chunk_size = 24*60*12\n","min_interval = 30"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:25:41.701738Z","iopub.status.busy":"2023-11-02T14:25:41.701477Z","iopub.status.idle":"2023-11-02T14:25:41.710799Z","shell.execute_reply":"2023-11-02T14:25:41.709922Z","shell.execute_reply.started":"2023-11-02T14:25:41.701715Z"},"trusted":true},"outputs":[{"data":{"text/plain":["259"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["useable_series_ids = [s for s in series_ids if s not in drop_series]\n","if CFG.DEMO_MODE:\n","    useable_series_ids = useable_series_ids[:75]\n","    \n","np.random.shuffle(useable_series_ids)\n","len(useable_series_ids)"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:25:41.712456Z","iopub.status.busy":"2023-11-02T14:25:41.712101Z","iopub.status.idle":"2023-11-02T14:25:41.7208Z","shell.execute_reply":"2023-11-02T14:25:41.719982Z","shell.execute_reply.started":"2023-11-02T14:25:41.712427Z"},"trusted":true},"outputs":[],"source":["def get_sigma(epoch):\n","    if epoch < 4:\n","        return 90\n","    elif epoch < 7:\n","        return 60\n","    return 36"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T14:25:41.72251Z","iopub.status.busy":"2023-11-02T14:25:41.722085Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Fold 0\n","Epoch 0, sigma = 90\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/207 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":[" 74%|███████▍  | 153/207 [11:08<03:02,  3.39s/it]"]}],"source":["EPOCHS = 20\n","loss_fct = nn.KLDivLoss(reduction='sum')\n","\n","for fold, valid_series_ids in enumerate(np.array_split(useable_series_ids, 5)):\n","    print(f'Fold {fold}')\n","    train_series_ids = [s for s in useable_series_ids if s not in valid_series_ids]\n","    net = GRUNET(arch=arch,out_channels=2, hidden_size=hidden_size, kernel_size=kernel_size, stride=stride, \n","                 dconv_padding=dconv_padding, n_layers=n_layers, bidir=True, print_shape=False).to(CFG.DEVICE)\n","    learning_rate = 1.e-3\n","    clip_val = 2.\n","    weight_decay=0.0\n","    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","\n","    WARMUP_PROP = 0.1\n","    train_size = len(train_series_ids)\n","    steps = train_size * EPOCHS\n","    warmup_steps = int(steps * WARMUP_PROP)\n","    scheduler = CosineLRScheduler(optimizer,t_initial= steps,warmup_t=warmup_steps, warmup_lr_init=1e-5,lr_min=1e-6,)\n","    m = nn.LogSoftmax(dim=0)\n","\n","    train_loss_history = []\n","    valid_loss_history = []\n","    learning_rate_history = []\n","    mAP_history = []\n","\n","    # with torch.autograd.detect_anomaly(check_nan=True):\n","    for epoch in range(EPOCHS):\n","        SIGMA = get_sigma(epoch)\n","        \n","        np.random.shuffle(train_series_ids)\n","        train_ds = SleepDatasetTrain(train_series_ids, events=train_events, len_mult=len_mult, continuous=continuous, sigma=SIGMA)\n","        valid_ds = SleepDatasetTrain(valid_series_ids, events=train_events, len_mult=len_mult, sigma=SIGMA)\n","        print(f'Epoch {epoch}, sigma = {SIGMA}')\n","        net.train()\n","        train_loss = 0\n","\n","        for i in tqdm(range(len(train_ds))):\n","            X, Y = train_ds[i]\n","            Y = Y.to(CFG.DEVICE, non_blocking=True)\n","            if not np.isfinite(Y.sum().cpu()):\n","                print(f'Nan Target {i}')\n","\n","            pred = torch.zeros(Y.shape).to(CFG.DEVICE, non_blocking=True)\n","            optimizer.zero_grad()\n","            scheduler.step(i+train_size*epoch)\n","            h = None\n","\n","            seq_len = X.shape[0]\n","            for j in range(0, seq_len, max_chunk_size):\n","                X_chunk = X[j: j + max_chunk_size].float().to(CFG.DEVICE, non_blocking=True)\n","                y_pred, h = net(X_chunk, h)\n","                h = [hi.detach() for hi in h]\n","                pred[j: j+max_chunk_size, :] = y_pred\n","\n","                del X_chunk, y_pred\n","\n","            if not np.isfinite(pred.sum().cpu().detach()):\n","                print(f'Nan Pred before logsoftmax {i}')\n","            pred = m(pred.float())\n","            if not np.isfinite(pred.sum().cpu().detach()):\n","                print(f'Nan Pred after logsoftmax {i}')\n","            loss = loss_fct(pred.float(), Y.float())\n","            loss.backward()\n","            train_loss += loss.item()\n","\n","            nn.utils.clip_grad_norm_(net.parameters(), max_norm=clip_val)\n","            optimizer.step()\n","\n","            del pred, loss, Y, X, h\n","            clean_memory()\n","        train_loss /= len(train_ds)\n","        print(f'Epoch {epoch} train loss = {train_loss}')\n","        train_loss_history.append(train_loss)\n","        print(f'Learning Rate = {optimizer.param_groups[0][\"lr\"]}')\n","        learning_rate_history.append(optimizer.param_groups[0][\"lr\"])\n","        print('Evaluate Validation Loss and mAP')\n","        net.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for i in tqdm(range(len(valid_ds))):\n","                X, Y = valid_ds[i]\n","                Y = Y.to(CFG.DEVICE, non_blocking=True)\n","                pred = torch.zeros(Y.shape).to(CFG.DEVICE, non_blocking=True)\n","\n","                h = None\n","\n","                seq_len = X.shape[0]\n","                for j in range(0, seq_len, max_chunk_size):\n","                    X_chunk = X[j: j + max_chunk_size].float().to(CFG.DEVICE, non_blocking=True)\n","                    y_pred, h = net(X_chunk, h)\n","                    h = [hi.detach() for hi in h]\n","                    pred[j: j+max_chunk_size, :] = y_pred\n","\n","                    del X_chunk, y_pred\n","                pred = m(pred.float())\n","                loss = loss_fct(pred.float(), Y.float())\n","                val_loss += loss.item()\n","                del pred, loss, Y, X, h\n","                clean_memory()\n","            val_loss /= len(valid_ds)\n","\n","            if epoch >= 1:\n","                all_df = []\n","                all_truth_df = []\n","                for i in tqdm(range(len(valid_ds))):\n","                    series_id = valid_ds.series_ids[i]\n","                    # print(series_id)\n","                    data = valid_ds.load_data(series_id)\n","                    res_df, act_df = compare_predictions(valid_ds, i, net)\n","                    res_df['step'] = data['step']\n","                    onset_pred = get_predictions(res_df, target='onset_val', SIGMA=SIGMA)\n","                    wakeup_pred = get_predictions(res_df, target='wakeup_val', SIGMA=SIGMA)\n","                    pred_df = pd.DataFrame(wakeup_pred + onset_pred, columns=['step', 'event', 'score'])\n","                    pred_df['series_id'] = series_id\n","                    pred_df['row_id'] = pred_df.index\n","                    pred_df = pred_df.sort_values(by='step').drop_duplicates(subset='step').reset_index(drop=True)\n","\n","                    all_df.append(pred_df)\n","                    all_truth_df.append(train_events[(train_events.series_id == series_id) & (train_events.step <= data.step.max()) & (train_events.step >= data.step.min())])\n","\n","                pred_df = pd.concat(all_df).reset_index(drop=True)\n","                pred_df['row_id'] = pred_df.index\n","                pred_df = pred_df[['row_id', 'series_id', 'step', 'event', 'score']]\n","                pred_df = pred_df.sort_values(by=['series_id', 'step'])\n","                pred_df.event = pred_df.event.map(lambda x: x.replace('_val', ''))\n","                #pred_df = renormalize(pred_df)\n","                truth_df = pd.concat(all_truth_df).reset_index(drop=True)\n","                if len(pred_df) > 0:\n","                    map_val = mapmetric.event_detection_ap(solution=truth_df, submission=pred_df[['series_id', 'step', 'event', 'score']], tolerances=tolerances)\n","                else:\n","                    print(f'Empty pred dataframe')\n","                    map_val = 0\n","\n","                tmp = [x for x in mAP_history if not np.isnan(x)]\n","                if len(tmp) > 0 and map_val > np.max(tmp):\n","                    torch.save(net.state_dict(), f'model_best_mAP{fold}.pth')\n","            else:\n","                map_val = np.nan\n","\n","        print(f'Epoch {epoch} validation loss = {val_loss}, mAP = {map_val}')\n","        valid_loss_history.append(val_loss)\n","        mAP_history.append(map_val)\n","        \n","    torch.save(net.state_dict(), f'model_resid_bigru_fold{fold}.pth')\n","    iplot({'data': [Scatter(y=train_loss_history, name='train'), Scatter(y=valid_loss_history, name='valid')], 'layout': Layout(title=f'KLDivLoss {fold}')})\n","    iplot({'data': [Scatter(y=learning_rate_history, name='lr')], 'layout': Layout(title=f'Learning Rate {fold}')})\n","    iplot({'data': [Scatter(y=mAP_history, name='mAP')], 'layout': Layout(title=f'Event mAP {fold}')})\n","    print('Break after 1 to save GPU!')\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["## Visualization\n","\n","We used plotly.  Zoom in to see how the predictions hold up!"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
